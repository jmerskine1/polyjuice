{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddfb7eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "is_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64cd17",
   "metadata": {},
   "source": [
    "# General setup and perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a wrapper.\n",
    "# model path is defaulted to our portable model:\n",
    "# https://huggingface.co/uw-hai/polyjuice\n",
    "# No need to change this unless you are using customized model\n",
    "from polyjuice import Polyjuice\n",
    "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c007e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:polyjuice.polyjuice_wrapper:Setup Polyjuice.\n",
      "Device set to use cpu\n",
      "INFO:polyjuice.polyjuice_wrapper:Setup SpaCy processor.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:polyjuice.polyjuice_wrapper:Setup perplexity scorer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is great to be.', \"It's not for kids.\", 'It is great for kids to watch.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the base sentence\n",
    "text = \"It is great for kids.\"\n",
    "\n",
    "# perturb the sentence with one line:\n",
    "# When running it for the first time, the wrapper will automatically\n",
    "# load related models, e.g. the generator and the perplexity filter.\n",
    "perturbations = pj.perturb(text)\n",
    "perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e3e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is great for kids but not for anyone.',\n",
       " 'It is great for kids but not for any adults.',\n",
       " 'It is not great for kids.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To perturb with more controls,\n",
    "\n",
    "perturbations = pj.perturb(\n",
    "    orig_sent=text,\n",
    "    # can specify where to put the blank. Otherwise, it's automatically selected.\n",
    "    # Can be a list or a single sentence.\n",
    "    blanked_sent=[\"It is [BLANK] for kids.\", \"It is great for [BLANK].\"],\n",
    "    # can also specify the ctrl code (a list or a single code.)\n",
    "    # The code should be from 'resemantic', 'restructure', 'negation', 'insert', 'lexical', 'shuffle', 'quantifier', 'delete'.\n",
    "    ctrl_code=\"negation\",\n",
    "    # Customzie perplexity score. \n",
    "    perplex_thred=20,\n",
    "    # number of perturbations to return\n",
    "    num_perturbations=3,\n",
    "    # the function also takes in additional arguments for huggingface generators.\n",
    "    num_beams=3\n",
    ")\n",
    "perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bac5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detect the ctrl code from a given sentence pair\n",
    "pj.detect_ctrl_code(\"it's great for kids.\", 'It is great for kids but not for any adults.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcc528",
   "metadata": {},
   "source": [
    "# Select for diversity\n",
    "\n",
    "Having each perturbation be represented by its token changes, control code, and dependency tree strcuture, we greedily select the ones that are least similar to those already selected. This tries to avoid redundancy in common perturbations such as black -> white.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793f1239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('It is great for kids.', 'It is bad for kids.'),\n",
       " ('It is great for kids.', 'It is good for kids too.'),\n",
       " ('It is great for kids.', 'It is terrible for kids.'),\n",
       " ('It is great for kids.', 'is great for kids.'),\n",
       " ('It is great for kids.', 'It is great for any audience.'),\n",
       " ('It is great for kids.', 'It is not for kids.'),\n",
       " ('It is great for kids.', 'It is good for kids.'),\n",
       " ('It is great for kids.', 'It is great for adults.'),\n",
       " ('It is great for kids.', 'It is great for anyone.'),\n",
       " ('It is great for kids.', 'It is not great for kids.'),\n",
       " ('It is great for kids.', 'It is not good for kids.'),\n",
       " ('It is great for kids.', 'It is great for any child.'),\n",
       " ('It is great for kids.', 'It is great for kids but not for adults.'),\n",
       " ('It is great for kids.', 'It is bad for kids too.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# over-generate some examples\n",
    "\n",
    "orig_text = \"It is great for kids.\"\n",
    "perturb_texts = pj.perturb(\n",
    "    orig_sent=orig_text, perplex_thred=10, num_perturbations=None, num_beams=3)\n",
    "orig_and_perturb_pairs = [(orig_text, perturb_text) for perturb_text in perturb_texts]\n",
    "orig_and_perturb_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85693442",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hypernyms() got an unexpected keyword argument 'recursive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sampled \u001b[38;5;241m=\u001b[39m \u001b[43mpj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_diverse_perturbations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_and_perturb_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_and_perturb_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m sampled\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/polyjuice_wrapper.py:294\u001b[0m, in \u001b[0;36mPolyjuice.select_diverse_perturbations\u001b[0;34m(self, orig_and_perturb_pairs, nsamples)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect_diverse_perturbations\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    277\u001b[0m     orig_and_perturb_pairs: List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m    278\u001b[0m     nsamples: \u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Having each perturbation be represented by its token changes, \u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    control code, and dependency tree strcuture, we greedily select the \u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    ones that are least similar to those already selected. This tries \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m        List[Tuple[str, str]]: A subsample of (orig, perturb) pairs.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_diverse_perturbations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43morig_and_perturb_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_and_perturb_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_sent_cosine_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_sent_cosine_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/filters_and_selectors/selections.py:72\u001b[0m, in \u001b[0;36mselect_diverse_perturbations\u001b[0;34m(orig_and_perturb_pairs, nsamples, compute_sent_cosine_distance, process)\u001b[0m\n\u001b[1;32m     70\u001b[0m eops \u001b[38;5;241m=\u001b[39m compute_edit_ops(orig_doc, perturb_doc)\n\u001b[1;32m     71\u001b[0m meta \u001b[38;5;241m=\u001b[39m SentenceMetadata(eops)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_similarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_sent_cosine_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mprimary:\n\u001b[1;32m     74\u001b[0m     metas\u001b[38;5;241m.\u001b[39mappend(meta)\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/compute_perturbs/compute_ctrl_meta.py:585\u001b[0m, in \u001b[0;36mSentenceMetadata.compute_metadata\u001b[0;34m(self, sentence_similarity, is_select_primary)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metadata\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_similarity, is_select_primary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphrases:\n\u001b[0;32m--> 585\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_similarity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m#print(p)\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_select_primary \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphrases:\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/compute_perturbs/compute_ctrl_meta.py:464\u001b[0m, in \u001b[0;36mPhraseMetadata.compute_metadata\u001b[0;34m(self, sentence_similarity, is_overwrite_local)\u001b[0m\n\u001b[1;32m    462\u001b[0m         tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestructure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_local \u001b[38;5;129;01mand\u001b[39;00m is_structure_remain:\n\u001b[0;32m--> 464\u001b[0m     hypers1, hypers2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_hypernyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43maroot\u001b[49m\u001b[43m)\u001b[49m, get_hypernyms(broot)\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m#print(aroot, hypers1)\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m#print(broot, hypers2)\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_wordnet_change(aroot, broot) \u001b[38;5;129;01mand\u001b[39;00m (acore\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m hypers2 \u001b[38;5;129;01mor\u001b[39;00m acore\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m hypers2):\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/compute_perturbs/compute_ctrl_meta.py:136\u001b[0m, in \u001b[0;36mget_hypernyms\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_hypernyms\u001b[39m(root):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_noun(root): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 136\u001b[0m     hypers \u001b[38;5;241m=\u001b[39m \u001b[43mget_wordnet_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_possible_hypernyms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m root\u001b[38;5;241m.\u001b[39mis_stop: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m root\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROPN\u001b[39m\u001b[38;5;124m\"\u001b[39m]: hypers\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msomething\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/compute_perturbs/compute_ctrl_meta.py:133\u001b[0m, in \u001b[0;36mget_wordnet_info\u001b[0;34m(func, span)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_wordnet_info\u001b[39m (func, span):\n\u001b[1;32m    132\u001b[0m     normalized_span \u001b[38;5;241m=\u001b[39m _normalize_span(span)\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_span\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(func(normalized_span\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower())))\n",
      "File \u001b[0;32m~/University of Bristol/polyjuice/polyjuice/compute_perturbs/compute_ctrl_meta.py:92\u001b[0m, in \u001b[0;36mall_possible_hypernyms\u001b[0;34m(word, pos, depth)\u001b[0m\n\u001b[1;32m     90\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m all_synsets(word, pos\u001b[38;5;241m=\u001b[39mpos):\n\u001b[0;32m---> 92\u001b[0m     ret\u001b[38;5;241m.\u001b[39mextend([y \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43msyn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypernyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msenses])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_senses(ret)\n",
      "\u001b[0;31mTypeError\u001b[0m: hypernyms() got an unexpected keyword argument 'recursive'"
     ]
    }
   ],
   "source": [
    "sampled = pj.select_diverse_perturbations(\n",
    "    orig_and_perturb_pairs=orig_and_perturb_pairs, nsamples=3)\n",
    "sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02b554",
   "metadata": {},
   "source": [
    "# Select surprising perturbations as counterfactual explanations\n",
    "\n",
    "Because different models/explainers may have different forms of predictions/feature weight computation methods, Polyjuice selection expects all predictions and feature weights to be precomputed. Here, we give an example of Quora Question Pair Detection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64272c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a perturbation base\n",
    "orig = (\n",
    "    \"How can I help a friend experiencing serious depression?\",\n",
    "    \"How do I help a friend who is in depression?\"\n",
    ")\n",
    "orig_label = 1\n",
    "\n",
    "# we perturb the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca52692",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_cuda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# sentiment analysis is a general name in Huggingface to load the pipeline for text classification tasks.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# set device=-1 if you don't have a gpu\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[0;32m---> 11\u001b[0m     framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_cuda\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, return_all_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_cuda' is not defined"
     ]
    }
   ],
   "source": [
    "# get a model\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "model_name = \"textattack/bert-base-uncased-QQP\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# sentiment analysis is a general name in Huggingface to load the pipeline for text classification tasks.\n",
    "# set device=-1 if you don't have a gpu\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\", model=model, tokenizer=tokenizer, \n",
    "    framework=\"pt\", device=0 if is_cuda else -1, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c43d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'label': 0, 'score': 0.0179734043776989},\n",
       "  {'label': 1, 'score': 0.9820265769958496}],\n",
       " 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some wrapper for prediction\n",
    "import numpy as np\n",
    "def extract_predict_label(raw_pred):\n",
    "    raw_pred = sorted(raw_pred, key=lambda r: -r[\"score\"])\n",
    "    if raw_pred:\n",
    "        return raw_pred[0][\"label\"]\n",
    "    return None\n",
    "def predict(examples, predictor, batch_size=128):\n",
    "    raw_preds, preds, distribution = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for e in (range(0, len(examples), batch_size)):\n",
    "            raw_preds.extend(predictor(examples[e:e+batch_size]))\n",
    "    for raw_pred in raw_preds:\n",
    "        raw_pred = raw_pred if type(raw_pred) == list else [raw_pred]\n",
    "        for m in raw_pred:\n",
    "            m[\"label\"] = int(m[\"label\"].split(\"_\")[1])\n",
    "    return raw_preds\n",
    "\n",
    "p = predict([orig], predictor=pipe)[0]\n",
    "(p, extract_predict_label(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893571aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do I help a suicidal girl?',\n",
       " 'How do I help a friend who is suicidal?',\n",
       " 'How do I get help a friend who is in depression?',\n",
       " 'How do I help a friend who is in really bad health?',\n",
       " 'How do I help a friend who is in deep trouble?',\n",
       " 'How would I help a friend who is in depression?',\n",
       " 'How do I help a friend who is in health?',\n",
       " 'How do I help a friend?',\n",
       " 'How do I help a suicidal student?',\n",
       " 'How do I not help a friend who is in depression?',\n",
       " 'How can I help a friend who is in depression?']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## collect some base perturbations\n",
    "from polyjuice.generations import ALL_CTRL_CODES\n",
    "\n",
    "# perturb the second question in orig.\n",
    "perturb_idx = 1\n",
    "perturb_texts = pj.perturb(\n",
    "    orig[perturb_idx], \n",
    "    ctrl_code=ALL_CTRL_CODES, \n",
    "    num_perturbations=None, perplex_thred=10)\n",
    "\n",
    "perturb_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1ab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: shap in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (0.39.0)\n",
      "Requirement already satisfied: numpy in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (1.20.2)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (4.60.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: scikit-learn in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (0.24.2)\n",
      "Requirement already satisfied: pandas in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (1.2.4)\n",
      "Requirement already satisfied: scipy in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (1.6.3)\n",
      "Requirement already satisfied: numba in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (0.53.1)\n",
      "Requirement already satisfied: cloudpickle in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from numba->shap) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from numba->shap) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from pandas->shap) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from pandas->shap) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from scikit-learn->shap) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/wtshuang/anaconda3/envs/polyjuice_env/lib/python3.7/site-packages (from scikit-learn->shap) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# To estimate feature importance, we set up shap explainer\n",
    "# install shap\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b6d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'How': 0.052321239694720134,\n",
       " 'do': 0.052321239694720134,\n",
       " 'I': 0.05254904864705168,\n",
       " 'help': 0.05254904864705168,\n",
       " 'a': 0.03752649684611242,\n",
       " 'friend': 0.03752649684611242,\n",
       " 'who': 0.03752649684611242,\n",
       " 'is': 0.03752649684611242,\n",
       " 'in': 0.2708918958087452,\n",
       " 'depression': 0.2708918958087452,\n",
       " '?': 0.07552210992434993}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shap\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "# setup a prediction function for computing the shap feature importance\n",
    "\n",
    "def wrap_perturbed_instances(perturb_texts, orig, perturb_idx=1):\n",
    "    perturbs = []\n",
    "    for a in perturb_texts:\n",
    "        curr_example = deepcopy(list(orig))\n",
    "        curr_example[perturb_idx] = a\n",
    "        perturbs.append(tuple(curr_example))\n",
    "    return perturbs\n",
    "\n",
    "def predict_on_perturbs(perturb_texts, orig, predictor, perturb_idx=1):\n",
    "    perturbs = wrap_perturbed_instances(perturb_texts, orig, perturb_idx)\n",
    "    perturbs_preds = predict(perturbs, predictor=predictor)\n",
    "    perturbs_pred_dicts = [{p[\"label\"]: p[\"score\"] for p in perturbs_pred} for perturbs_pred in perturbs_preds]\n",
    "    orig_preds = predict([orig], predictor=predictor)\n",
    "    orig_pred = extract_predict_label(orig_preds[0])\n",
    "    # the return is probability of the originally predicted label\n",
    "    return [pr_dict[orig_pred] for pr_dict in perturbs_pred_dicts]\n",
    "def normalize_shap_importance(features, importances, is_use_abs=True):\n",
    "    normalized_features = {}\n",
    "    for idx, (f, v) in enumerate(zip(features, importances)):\n",
    "        f = f.strip('Ġ')\n",
    "        if not f.startswith(\"##\"): \n",
    "            key, val = \"\", 0\n",
    "        key += f.replace(\"#\", \"\").strip()\n",
    "        val += v\n",
    "        if (idx == len(features)-1 or (not features[idx+1].startswith(\"##\"))) and key != \"\":\n",
    "            normalized_features[key] = abs(val) if is_use_abs else val\n",
    "    return normalized_features\n",
    "def explain_with_shap(orig, predictor=pipe, tokenzier=pipe.tokenizer, perturb_idx=1):\n",
    "    predict_for_shap_func = functools.partial(\n",
    "        predict_on_perturbs, orig=orig, predictor=predictor, perturb_idx=perturb_idx)\n",
    "    shap_explainer = shap.Explainer(predict_for_shap_func, tokenizer) \n",
    "    exp = shap_explainer([str(orig[perturb_idx])])\n",
    "    return normalize_shap_importance(exp.data[0], exp.values[0])\n",
    "\n",
    "feature_importance_dict = explain_with_shap(orig)\n",
    "feature_importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569debb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Munch({'case': 'Suprise flip', 'pred': 0, 'changed_features': ['help'], 'perturb_text': 'How do I not help a friend who is in depression?'}),\n",
       " Munch({'case': 'Suprise unflip', 'pred': 1, 'changed_features': ['depression'], 'perturb_text': 'How do I help a friend who is in really bad health?'})]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the predictions for original and also new instances\n",
    "orig_pred = predict([orig], predictor=pipe)[0]\n",
    "\n",
    "perturb_instances = wrap_perturbed_instances(perturb_texts, orig, perturb_idx)\n",
    "perturb_preds = predict(perturb_instances, predictor=pipe)\n",
    "\n",
    "surprises = pj.select_surprise_explanations(\n",
    "    orig_text=orig[perturb_idx], \n",
    "    perturb_texts=perturb_texts, \n",
    "    orig_pred=orig_pred, \n",
    "    perturb_preds=perturb_preds, \n",
    "    feature_importance_dict=feature_importance_dict)\n",
    "surprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3215f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
